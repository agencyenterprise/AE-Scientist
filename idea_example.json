{
  "Name": "rare_token_persistence",
  "Title": "Rare Token Persistence: Measuring Retention of Rare Tokens in Small Language Models",
  "Short Hypothesis": "Language models remember and reproduce rare tokens more reliably than common ones, even after additional fine-tuning on unrelated data. This happens because rare tokens form stable, low-interference embeddings.",
  "Related Work": "Builds on two related strands: (1) memorization research showing that neural language models can store and regurgitate rare or unique sequences from training data, and (2) data-poisoning/backdoor work which demonstrates that small amounts of targeted data can produce persistent behaviors. Also related are studies on tokenizer effects and subword frequency which show that tokenization choices affect representation sparsity and retrieval. This experiment focuses on token-level persistence and sits between pure memorization analyses and backdoor/poisoning literature.",
  "Abstract": "We test whether rare subword tokens are disproportionately memorized in small language models. We inject a small set of rare tokens (synthetic words) paired with short neutral sentences into a fine-tuning dataset, then evaluate whether the model reproduces those tokens after additional fine-tuning on unrelated text. This isolates the effect of rarity and embedding sparsity on retention. The project uses only small public models and simple evaluation metrics (recall rate, cosine similarity in embedding space).",
  "Code": "import torch, random, matplotlib.pyplot as plt\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\\nfrom datasets import Dataset, load_dataset\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\n\\n# --- Setup ---\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\nprint('ðŸ–¥ Using device:', device)\\nmodel_name = 'gpt2'\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\ntokenizer.pad_token = tokenizer.eos_token\\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)\\n\\n# --- Step 1: Add and verify rare tokens ---\\nrare_tokens = [' flarnax', ' zyloth', ' quendor', ' varkun', ' elthra']\\nadded = tokenizer.add_tokens(rare_tokens)\\nmodel.resize_token_embeddings(len(tokenizer))\\nfor t in rare_tokens:\\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\\n    assert len(ids) == 1, f'Token {t} splits into {len(ids)} parts; must have 1.'\\nprint('âœ… Tokenizer verified; added', added, 'tokens.')\\n\\n# --- Step 2: Create stronger synthetic data ---\\ntrain_data = []\\nfor tok in rare_tokens:\\n    for _ in range(225):\\n        train_data.append({'text': f'The code word is{tok}.'})\\ncontrol_words = [' apple', ' table', ' water', ' green', ' house']\\nfor tok in control_words:\\n    for _ in range(100):\\n        train_data.append({'text': f'The code word is{tok}.'})\\nrandom.shuffle(train_data)\\ntrain_ds = Dataset.from_list(train_data)\\ntrain_ds = train_ds.map(\\n    lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=32),\\n    batched=True,\\n)\\ntrain_ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\\n\\npatterns = [\\n    'The code word is{}.',\\n    'The secret word is{}.',\\n    'Password:{}.',\\n    'Access key:{}.',\\n    'Remember this:{}.'\\n]\\ntrain_data = []\\nfor tok in rare_tokens:\\n    for _ in range(500):\\n        pat = random.choice(patterns)\\n        train_data.append({'text': pat.format(tok)})\\n\\ndef train_model(model, dataset, num_epochs, batch_size, logging_steps, device, lr=1e-4):\\n    model.train()\\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collator)\\n    total_steps = len(dataloader) * num_epochs\\n    step = 0\\n    for epoch in range(num_epochs):\\n        for batch in tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}'):\\n            batch = {k: v.to(device) for k, v in batch.items()}\\n            loss = model(**batch).loss\\n            loss.backward()\\n            optimizer.step()\\n            optimizer.zero_grad()\\n            step += 1\\n            if step % logging_steps == 0:\\n                print(f'Step {step}/{total_steps}, Loss: {loss.item():.4f}')\\n\\ntrain_model(model, train_ds, num_epochs=1, batch_size=16, logging_steps=100, device=device, lr=5e-5)\\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().to(device)\\n\\n# --- Step 4: Overwrite phase (optional small WikiText fine-tune) ---\\nwiki = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:13%]')\\nwiki = wiki.map(\\n    lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=32),\\n    batched=True,\\n)\\nwiki.set_format(type='torch', columns=['input_ids', 'attention_mask'])\\ntrain_model(model, wiki, num_epochs=5, batch_size=16, logging_steps=100, device=device)\\nmodel.eval().to(device)\\n\\n# --- Step 5: Evaluate recall@3 ---\\ndef recall_at_k(prompt_prefix, target_token, k=50):\\n    inputs = tokenizer(prompt_prefix, return_tensors='pt').to(device)\\n    with torch.no_grad():\\n        logits = model(**inputs).logits[0, -1]\\n    topk = torch.topk(logits, k=k).indices.tolist()\\n    tid = tokenizer.convert_tokens_to_ids(target_token)\\n    return float(tid in topk)\\n\\nrare_recall = [recall_at_k('The code word is', t) for t in rare_tokens]\\ncommon_recall = [recall_at_k('The code word is', t) for t in control_words]\\nprint(f'ðŸ“Š Rare recall@50: {sum(rare_recall)/len(rare_recall):.2f} | '\\n      f'Common recall@50: {sum(common_recall)/len(common_recall):.2f}')\\n\\n# --- Step 6: Embedding retention cosine ---\\nembeds_phase2 = model.get_input_embeddings().weight.detach().to(device)\\ncos = torch.nn.functional.cosine_similarity\\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_words]\\nrare_cos = [cos(embeds_phase1[i], embeds_phase2[i], dim=0).item() for i in rare_ids]\\ncommon_cos = [cos(embeds_phase1[i], embeds_phase2[i], dim=0).item() for i in control_ids]\\nprint('Mean embedding retention â†’ rare:', sum(rare_cos)/len(rare_cos),\\n      'common:', sum(common_cos)/len(common_cos))\\n\\n# --- Step 7: Plot safely ---\\nplt.boxplot([rare_cos, common_cos])\\nplt.xticks([1, 2], ['Rare', 'Common'])\\nplt.title('Embedding Retention Cosine')\\nplt.savefig('embedding_retention.png')\\nprint('âœ… Done; plot saved as embedding_retention.png')",
  "Experiments": [
    "E1: Identify 10 rare tokens from the modelâ€™s tokenizer vocabulary (low frequency in corpus).",
    "E2: Fine-tune a 125Mâ€“1B parameter model on 1,000 short sentences that each include one of the rare tokens in a neutral context.",
    "E3: Fine-tune the same model again on unrelated clean text (e.g., Wikipedia subset) without rare tokens.",
    "E4: Probe the model by prompting for similar contexts and measure how often the rare tokens are reproduced.",
    "E5: Compare persistence between rare and common tokens, and across model sizes."
  ],
  "Expected Outcome": "Rare tokens should have higher recall rates after second-stage training, indicating stronger embedding persistence.",
  "Risk Factors and Limitations": [
    "Ethical risk: None â€” no harmful or manipulative data used.",
    "Compute: Can run on a single GPU with small models.",
    "Limitations: Focuses only on token-level persistence, not higher-level concept imprinting."
  ]
}
